import os
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# Carrega as chaves do .env
load_dotenv()

# --- CONFIGURA√á√ÉO ---
ARQUIVO_EMAILS = "emails.txt"  # Nome exato do seu arquivo enviado
DIRETORIO_DB = "./db_emails"   # Pasta separada para a investiga√ß√£o

def realizar_investigacao():
    print("üïµÔ∏è‚Äç‚ôÇÔ∏è  Iniciando Protocolo de Investiga√ß√£o 'Toby-Holmes'...")

    # 1. Carregar e Processar o Dump de E-mails
    if not os.path.exists(ARQUIVO_EMAILS):
        raise FileNotFoundError(f"O arquivo {ARQUIVO_EMAILS} n√£o foi encontrado!")

    print(f"üìÇ Lendo evid√™ncias em '{ARQUIVO_EMAILS}'...")
    with open(ARQUIVO_EMAILS, "r", encoding="utf-8") as f:
        texto_emails = f.read()

    # Divis√£o: O seu arquivo usa uma linha de tra√ßos como separador
    # Vamos usar isso para garantir que cada e-mail seja um documento separado
    text_splitter = CharacterTextSplitter(
        separator="-------------------------------------------------------------------------------",
        chunk_size=1500,  # Tamanho seguro para pegar um email inteiro
        chunk_overlap=0
    )
    documentos = text_splitter.create_documents([texto_emails])
    print(f"üìÑ E-mails processados: {len(documentos)}")

    # 2. Criar Embeddings (O "C√©rebro" da busca)
    print("üß† Criando conex√µes neurais (Indexando e-mails)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 3. Armazenar no Banco Vetorial (Persistente)
    # Se a pasta j√° existir, ele carrega; se n√£o, ele cria.
    if os.path.exists(DIRETORIO_DB):
        print("üíæ Carregando banco de e-mails existente...")
        vectorstore = Chroma(persist_directory=DIRETORIO_DB, embedding_function=embeddings)
    else:
        print("üíæ Criando novo banco de dados forense...")
        vectorstore = Chroma.from_documents(
            documents=documentos,
            embedding=embeddings,
            persist_directory=DIRETORIO_DB
        )

    # 4. Configurar o Detetive (LLM)
    # Usando o modelo Llama 3.1 atualizado
    chat_model = ChatGroq(model_name="llama-3.1-8b-instant")

    # Prompt focado em investiga√ß√£o e cita√ß√£o de provas
    template_investigacao = """
    Voc√™ √© um investigador forense analisando e-mails corporativos da Dunder Mifflin.
    
    Contexto (E-mails recuperados):
    {context}

    Pergunta da Investiga√ß√£o: {question}

    Instru√ß√µes:
    1. Responda se a suspeita √© verdadeira ou falsa baseada APENAS no texto.
    2. Se encontrar provas, cite: QUEM enviou, PARA QUEM e o ASSUNTO.
    3. Seja direto e profissional, como um relat√≥rio policial.
    """

    prompt = ChatPromptTemplate.from_template(template_investigacao)
    chain = prompt | chat_model

    # 5. A Investiga√ß√£o (O Toby quer saber se o Michael est√° armando contra ele)
    pergunta_investigacao = "O Michael Scott est√° conspirando contra o Toby Flenderson? Existem planos de demiss√£o, armadilhas ou opera√ß√µes secretas mencionadas?"

    print(f"\nüîç Buscando respostas para: '{pergunta_investigacao}'")
    
    # Busca os 4 e-mails mais incriminadores
    docs_relacionados = vectorstore.similarity_search(pergunta_investigacao, k=4)
    contexto = "\n\n".join([doc.page_content for doc in docs_relacionados])

    resposta = chain.invoke({"context": contexto, "question": pergunta_investigacao})

    print("\n" + "="*40)
    print("üìã RELAT√ìRIO FINAL DE INVESTIGA√á√ÉO")
    print("="*40)
    print(resposta.content)

if __name__ == "__main__":
    realizar_investigacao()